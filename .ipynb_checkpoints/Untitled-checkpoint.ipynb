{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3328e4ee-da6d-45a2-94fc-519a2b9028bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR-10 데이터셋을 로드하는 중입니다...\n",
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 0us/step\n",
      "데이터셋 로드 완료.\n",
      "학습 데이터 형태: (5000, 3072)\n",
      "테스트 데이터 형태: (1000, 3072)\n",
      "k=7인 k-NN 모델 학습을 시작합니다. (시간이 다소 소요될 수 있습니다)\n",
      "학습 완료! 소요 시간: 0.02초\n",
      "테스트 데이터에 대한 예측을 시작합니다...\n",
      "예측 완료! 소요 시간: 0.25초\n",
      "모델 정확도: 27.40%\n",
      "예측 결과를 CSV 파일로 저장하는 중입니다...\n",
      "'cifar10_predictions.csv' 파일로 결과가 성공적으로 저장되었습니다.\n",
      "\n",
      "=== 미리보기 ===\n",
      "   id label\n",
      "0   1  deer\n",
      "1   2  ship\n",
      "2   3  ship\n",
      "3   4  ship\n",
      "4   5  deer\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# 1. CIFAR-10 데이터셋 로드\n",
    "print(\"CIFAR-10 데이터셋을 로드하는 중입니다...\")\n",
    "(X_train_orig, y_train_orig), (X_test_orig, y_test_orig) = tf.keras.datasets.cifar10.load_data()\n",
    "print(\"데이터셋 로드 완료.\")\n",
    "\n",
    "# 클래스 이름 정의\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# 2. 데이터 전처리 및 축소\n",
    "# k-NN은 계산량이 많으므로, 전체 데이터셋 대신 일부 샘플을 사용하여 시연합니다.\n",
    "# (실제 과제에서는 아래 주석을 풀고 전체 데이터로 학습할 수 있습니다)\n",
    "num_train_samples = 5000  # 학습용 데이터 개수 (최대 50000)\n",
    "num_test_samples = 1000   # 테스트용 데이터 개수 (최대 10000)\n",
    "\n",
    "X_train = X_train_orig[:num_train_samples]\n",
    "y_train = y_train_orig[:num_train_samples]\n",
    "X_test = X_test_orig[:num_test_samples]\n",
    "\n",
    "# 이미지를 1차원 벡터로 변환 (32x32x3 -> 3072)\n",
    "X_train_flattened = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_flattened = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# 픽셀 값을 0과 1 사이로 정규화\n",
    "X_train_normalized = X_train_flattened / 255.0\n",
    "X_test_normalized = X_test_flattened / 255.0\n",
    "\n",
    "# 레이블을 1차원 배열로 변환\n",
    "y_train_flattened = y_train.ravel()\n",
    "\n",
    "print(f\"학습 데이터 형태: {X_train_normalized.shape}\")\n",
    "print(f\"테스트 데이터 형태: {X_test_normalized.shape}\")\n",
    "\n",
    "# 3. k-NN 모델 생성 및 학습\n",
    "# 이웃의 수(k)를 7로 설정합니다. k는 성능에 영향을 미치는 중요한 파라미터입니다.\n",
    "k = 7\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=k, n_jobs=-1) # n_jobs=-1은 모든 CPU 코어를 사용합니다.\n",
    "\n",
    "print(f\"k={k}인 k-NN 모델 학습을 시작합니다. (시간이 다소 소요될 수 있습니다)\")\n",
    "start_time = time.time()\n",
    "knn_classifier.fit(X_train_normalized, y_train_flattened)\n",
    "end_time = time.time()\n",
    "print(f\"학습 완료! 소요 시간: {end_time - start_time:.2f}초\")\n",
    "\n",
    "\n",
    "# 4. 테스트 데이터에 대한 예측 수행\n",
    "print(\"테스트 데이터에 대한 예측을 시작합니다...\")\n",
    "start_time = time.time()\n",
    "y_pred = knn_classifier.predict(X_test_normalized)\n",
    "end_time = time.time()\n",
    "print(f\"예측 완료! 소요 시간: {end_time - start_time:.2f}초\")\n",
    "\n",
    "\n",
    "# (선택) 모델 정확도 평가\n",
    "# 실제 레이블(y_test_orig)이 있다면 정확도를 계산할 수 있습니다.\n",
    "accuracy = accuracy_score(y_test_orig[:num_test_samples], y_pred)\n",
    "print(f\"모델 정확도: {accuracy * 100:.2f}%\")\n",
    "\n",
    "\n",
    "# 5. 예측 결과를 CSV 파일로 저장\n",
    "print(\"예측 결과를 CSV 파일로 저장하는 중입니다...\")\n",
    "# 예측된 숫자 레이블을 실제 클래스 이름으로 변환\n",
    "predicted_labels = [class_names[i] for i in y_pred]\n",
    "\n",
    "# id는 1부터 시작하도록 생성\n",
    "ids = np.arange(1, len(predicted_labels) + 1)\n",
    "\n",
    "# Pandas DataFrame 생성\n",
    "results_df = pd.DataFrame({\n",
    "    'id': ids,\n",
    "    'label': predicted_labels\n",
    "})\n",
    "\n",
    "# CSV 파일로 저장\n",
    "output_filename = 'cifar10_predictions.csv'\n",
    "results_df.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"'{output_filename}' 파일로 결과가 성공적으로 저장되었습니다.\")\n",
    "print(\"\\n=== 미리보기 ===\")\n",
    "print(results_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ad7e18b-c05b-4b73-8dfd-050900794861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR-10 전체 데이터셋을 로드하는 중입니다...\n",
      "데이터셋 로드 완료.\n",
      "------------------------------\n",
      "데이터 전처리를 시작합니다...\n",
      "전체 학습 데이터 형태: (50000, 3072)\n",
      "전체 테스트 데이터 형태: (10000, 3072)\n",
      "데이터 전처리 완료.\n",
      "------------------------------\n",
      "최적의 k를 찾기 위해 검증을 시작합니다...\n",
      "k = 1 모델 테스트 중...\n",
      "k = 1일 때의 검증 정확도: 33.66%\n",
      "k = 2 모델 테스트 중...\n",
      "k = 2일 때의 검증 정확도: 30.52%\n",
      "k = 3 모델 테스트 중...\n",
      "k = 3일 때의 검증 정확도: 32.16%\n",
      "k = 4 모델 테스트 중...\n",
      "k = 4일 때의 검증 정확도: 33.78%\n",
      "k = 5 모델 테스트 중...\n",
      "k = 5일 때의 검증 정확도: 34.16%\n",
      "k = 6 모델 테스트 중...\n",
      "k = 6일 때의 검증 정확도: 33.48%\n",
      "k = 7 모델 테스트 중...\n",
      "k = 7일 때의 검증 정확도: 33.56%\n",
      "k = 8 모델 테스트 중...\n",
      "k = 8일 때의 검증 정확도: 33.32%\n",
      "k = 9 모델 테스트 중...\n",
      "k = 9일 때의 검증 정확도: 33.16%\n",
      "k = 10 모델 테스트 중...\n",
      "k = 10일 때의 검증 정확도: 32.98%\n",
      "k = 11 모델 테스트 중...\n",
      "k = 11일 때의 검증 정확도: 32.62%\n",
      "k = 12 모델 테스트 중...\n",
      "k = 12일 때의 검증 정확도: 32.52%\n",
      "k = 13 모델 테스트 중...\n",
      "k = 13일 때의 검증 정확도: 32.44%\n",
      "------------------------------\n",
      "최적의 k를 찾았습니다: k = 5 (검증 정확도: 34.16%)\n",
      "------------------------------\n",
      "최적의 k=5를 사용하여 전체 학습 데이터(50,000개)로 최종 모델을 학습합니다.\n",
      "(시간이 매우 오래 걸릴 수 있습니다. 컴퓨터 성능에 따라 수십 분 이상 소요될 수 있습니다.)\n",
      "최종 모델 학습 완료! 소요 시간: 0.35초\n",
      "------------------------------\n",
      "테스트 데이터(10,000개)에 대한 예측을 시작합니다...\n",
      "(이 과정 역시 시간이 매우 오래 걸립니다.)\n",
      "최종 예측 완료! 소요 시간: 19.99초\n",
      "------------------------------\n",
      "예측 결과를 CSV 파일로 저장합니다...\n",
      "'cifar10_predictions_final.csv' 파일로 결과가 성공적으로 저장되었습니다.\n",
      "\n",
      "=== 최종 결과 미리보기 ===\n",
      "   id     label\n",
      "0   1      bird\n",
      "1   2      ship\n",
      "2   3      ship\n",
      "3   4  airplane\n",
      "4   5      deer\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# 1. CIFAR-10 전체 데이터셋 로드\n",
    "print(\"CIFAR-10 전체 데이터셋을 로드하는 중입니다...\")\n",
    "(X_train_orig, y_train_orig), (X_test_orig, y_test_orig) = tf.keras.datasets.cifar10.load_data()\n",
    "print(\"데이터셋 로드 완료.\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 클래스 이름 정의\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# 2. 데이터 전처리\n",
    "print(\"데이터 전처리를 시작합니다...\")\n",
    "# 학습 및 테스트 이미지를 1차원 벡터로 변환\n",
    "X_train_flattened = X_train_orig.reshape(X_train_orig.shape[0], -1)\n",
    "X_test_flattened = X_test_orig.reshape(X_test_orig.shape[0], -1)\n",
    "\n",
    "# 픽셀 값을 0과 1 사이로 정규화\n",
    "X_train_normalized = X_train_flattened / 255.0\n",
    "X_test_normalized = X_test_flattened / 255.0\n",
    "\n",
    "# 레이블을 1차원 배열로 변환\n",
    "y_train_flattened = y_train_orig.ravel()\n",
    "\n",
    "print(f\"전체 학습 데이터 형태: {X_train_normalized.shape}\")\n",
    "print(f\"전체 테스트 데이터 형태: {X_test_normalized.shape}\")\n",
    "print(\"데이터 전처리 완료.\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# 3. 최적의 k를 찾기 위한 검증 과정\n",
    "print(\"최적의 k를 찾기 위해 검증을 시작합니다...\")\n",
    "# 학습 데이터를 다시 학습용과 검증용으로 분할 (예: 90% 학습, 10% 검증)\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    X_train_normalized, y_train_flattened, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "best_k = 0\n",
    "best_accuracy = 0\n",
    "k_values = range(1, 14) # k를 1부터 13까지 테스트\n",
    "\n",
    "for k in k_values:\n",
    "    print(f\"k = {k} 모델 테스트 중...\")\n",
    "    knn_temp = KNeighborsClassifier(n_neighbors=k, n_jobs=-1)\n",
    "    \n",
    "    # 학습\n",
    "    knn_temp.fit(X_train_split, y_train_split)\n",
    "    \n",
    "    # 검증 데이터로 예측 및 평가\n",
    "    y_val_pred = knn_temp.predict(X_val_split)\n",
    "    accuracy = accuracy_score(y_val_split, y_val_pred)\n",
    "    print(f\"k = {k}일 때의 검증 정확도: {accuracy * 100:.2f}%\")\n",
    "    \n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_k = k\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"최적의 k를 찾았습니다: k = {best_k} (검증 정확도: {best_accuracy * 100:.2f}%)\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# 4. 전체 학습 데이터로 최종 모델 학습\n",
    "print(f\"최적의 k={best_k}를 사용하여 전체 학습 데이터(50,000개)로 최종 모델을 학습합니다.\")\n",
    "print(\"(시간이 매우 오래 걸릴 수 있습니다. 컴퓨터 성능에 따라 수십 분 이상 소요될 수 있습니다.)\")\n",
    "\n",
    "final_knn_classifier = KNeighborsClassifier(n_neighbors=best_k, n_jobs=-1)\n",
    "\n",
    "start_time = time.time()\n",
    "final_knn_classifier.fit(X_train_normalized, y_train_flattened)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"최종 모델 학습 완료! 소요 시간: {end_time - start_time:.2f}초\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# 5. 테스트 데이터에 대한 최종 예측 수행\n",
    "print(\"테스트 데이터(10,000개)에 대한 예측을 시작합니다...\")\n",
    "print(\"(이 과정 역시 시간이 매우 오래 걸립니다.)\")\n",
    "\n",
    "start_time = time.time()\n",
    "final_predictions_numeric = final_knn_classifier.predict(X_test_normalized)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"최종 예측 완료! 소요 시간: {end_time - start_time:.2f}초\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# 6. 예측 결과를 CSV 파일로 저장\n",
    "print(\"예측 결과를 CSV 파일로 저장합니다...\")\n",
    "predicted_labels = [class_names[i] for i in final_predictions_numeric]\n",
    "ids = np.arange(1, len(predicted_labels) + 1)\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'id': ids,\n",
    "    'label': predicted_labels\n",
    "})\n",
    "\n",
    "output_filename = 'cifar10_predictions_final.csv'\n",
    "results_df.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"'{output_filename}' 파일로 결과가 성공적으로 저장되었습니다.\")\n",
    "print(\"\\n=== 최종 결과 미리보기 ===\")\n",
    "print(results_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "904aafa7-5985-48c0-bd94-827e6c36564a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR-10 전체 데이터셋을 로드하는 중입니다...\n",
      "데이터셋 로드 완료.\n",
      "------------------------------\n",
      "데이터 전처리를 시작합니다...\n",
      "데이터 전처리 완료.\n",
      "------------------------------\n",
      "PCA를 이용한 특징 추출을 시작합니다...\n",
      "PCA 변환 완료! (소요 시간: 11.38초)\n",
      "원본 차원: 3072, 축소된 차원: 150\n",
      "------------------------------\n",
      "최적의 k를 찾기 위해 검증을 시작합니다 (홀수 k만 테스트)...\n",
      "k = 1 모델 테스트 중...\n",
      "k = 1일 때의 검증 정확도: 36.74% (소요 시간: 0.58초)\n",
      "k = 3 모델 테스트 중...\n",
      "k = 3일 때의 검증 정확도: 35.82% (소요 시간: 0.55초)\n",
      "k = 5 모델 테스트 중...\n",
      "k = 5일 때의 검증 정확도: 36.66% (소요 시간: 0.46초)\n",
      "k = 7 모델 테스트 중...\n",
      "k = 7일 때의 검증 정확도: 36.52% (소요 시간: 0.45초)\n",
      "k = 9 모델 테스트 중...\n",
      "k = 9일 때의 검증 정확도: 36.48% (소요 시간: 0.47초)\n",
      "k = 11 모델 테스트 중...\n",
      "k = 11일 때의 검증 정확도: 36.28% (소요 시간: 0.47초)\n",
      "k = 13 모델 테스트 중...\n",
      "k = 13일 때의 검증 정확도: 35.98% (소요 시간: 0.48초)\n",
      "------------------------------\n",
      "최적의 k를 찾았습니다: k = 1 (검증 정확도: 36.74%)\n",
      "------------------------------\n",
      "최적의 k=1를 사용하여 전체 PCA 변환 데이터로 최종 모델을 학습합니다.\n",
      "최종 모델 학습 완료! (소요 시간: 0.00초)\n",
      "------------------------------\n",
      "PCA 변환된 테스트 데이터에 대한 최종 예측을 시작합니다...\n",
      "최종 예측 완료! (소요 시간: 0.93초)\n",
      "------------------------------\n",
      "최종 모델의 테스트 정확도: 37.84%\n",
      "------------------------------\n",
      "예측 결과를 CSV 파일로 저장합니다...\n",
      "'cifar10_predictions_pca_knn.csv' 파일로 결과가 성공적으로 저장되었습니다.\n",
      "\n",
      "=== 최종 결과 미리보기 ===\n",
      "   id     label\n",
      "0   1      deer\n",
      "1   2      ship\n",
      "2   3      ship\n",
      "3   4  airplane\n",
      "4   5      frog\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# 1. CIFAR-10 전체 데이터셋 로드\n",
    "print(\"CIFAR-10 전체 데이터셋을 로드하는 중입니다...\")\n",
    "(X_train_orig, y_train_orig), (X_test_orig, y_test_orig) = tf.keras.datasets.cifar10.load_data()\n",
    "print(\"데이터셋 로드 완료.\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 클래스 이름 정의\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# 2. 데이터 전처리\n",
    "print(\"데이터 전처리를 시작합니다...\")\n",
    "# 학습 및 테스트 이미지를 1차원 벡터로 변환\n",
    "X_train_flattened = X_train_orig.reshape(X_train_orig.shape[0], -1)\n",
    "X_test_flattened = X_test_orig.reshape(X_test_orig.shape[0], -1)\n",
    "\n",
    "# 픽셀 값을 0과 1 사이로 정규화\n",
    "X_train_normalized = X_train_flattened / 255.0\n",
    "X_test_normalized = X_test_flattened / 255.0\n",
    "\n",
    "# 레이블을 1차원 배열로 변환\n",
    "y_train_flattened = y_train_orig.ravel()\n",
    "print(\"데이터 전처리 완료.\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 3. PCA를 이용한 특징 추출 및 차원 축소\n",
    "print(\"PCA를 이용한 특징 추출을 시작합니다...\")\n",
    "# n_components는 줄이고자 하는 차원의 수입니다. 150은 경험적으로 좋은 성능을 내는 값 중 하나입니다.\n",
    "pca = PCA(n_components=150)\n",
    "\n",
    "# ★ 중요: PCA는 반드시 학습 데이터에만 fit 해야 합니다.\n",
    "start_time = time.time()\n",
    "pca.fit(X_train_normalized)\n",
    "\n",
    "# 학습 데이터와 테스트 데이터를 변환\n",
    "X_train_pca = pca.transform(X_train_normalized)\n",
    "X_test_pca = pca.transform(X_test_normalized)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"PCA 변환 완료! (소요 시간: {end_time - start_time:.2f}초)\")\n",
    "print(f\"원본 차원: {X_train_normalized.shape[1]}, 축소된 차원: {X_train_pca.shape[1]}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# 4. 최적의 k를 찾기 위한 검증 과정 (홀수 k만 사용)\n",
    "print(\"최적의 k를 찾기 위해 검증을 시작합니다 (홀수 k만 테스트)...\")\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    X_train_pca, y_train_flattened, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "best_k = 0\n",
    "best_accuracy = 0\n",
    "k_values = range(1, 14, 2)  # 1, 3, 5, 7, 9, 11, 13\n",
    "\n",
    "for k in k_values:\n",
    "    print(f\"k = {k} 모델 테스트 중...\")\n",
    "    knn_temp = KNeighborsClassifier(n_neighbors=k, n_jobs=-1)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    knn_temp.fit(X_train_split, y_train_split)\n",
    "    y_val_pred = knn_temp.predict(X_val_split)\n",
    "    accuracy = accuracy_score(y_val_split, y_val_pred)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"k = {k}일 때의 검증 정확도: {accuracy * 100:.2f}% (소요 시간: {end_time - start_time:.2f}초)\")\n",
    "    \n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_k = k\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"최적의 k를 찾았습니다: k = {best_k} (검증 정확도: {best_accuracy * 100:.2f}%)\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# 5. 전체 학습 데이터로 최종 모델 학습\n",
    "print(f\"최적의 k={best_k}를 사용하여 전체 PCA 변환 데이터로 최종 모델을 학습합니다.\")\n",
    "final_knn_classifier = KNeighborsClassifier(n_neighbors=best_k, n_jobs=-1)\n",
    "\n",
    "start_time = time.time()\n",
    "final_knn_classifier.fit(X_train_pca, y_train_flattened)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"최종 모델 학습 완료! (소요 시간: {end_time - start_time:.2f}초)\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# 6. 테스트 데이터에 대한 최종 예측 수행\n",
    "print(\"PCA 변환된 테스트 데이터에 대한 최종 예측을 시작합니다...\")\n",
    "start_time = time.time()\n",
    "final_predictions_numeric = final_knn_classifier.predict(X_test_pca)\n",
    "end_time = time.time()\n",
    "print(f\"최종 예측 완료! (소요 시간: {end_time - start_time:.2f}초)\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# (선택) 실제 테스트 데이터셋에 대한 정확도 평가\n",
    "final_accuracy = accuracy_score(y_test_orig.ravel(), final_predictions_numeric)\n",
    "print(f\"최종 모델의 테스트 정확도: {final_accuracy * 100:.2f}%\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 7. 예측 결과를 CSV 파일로 저장\n",
    "print(\"예측 결과를 CSV 파일로 저장합니다...\")\n",
    "predicted_labels = [class_names[i] for i in final_predictions_numeric]\n",
    "ids = np.arange(1, len(predicted_labels) + 1)\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'id': ids,\n",
    "    'label': predicted_labels\n",
    "})\n",
    "\n",
    "output_filename = 'cifar10_predictions_pca_knn.csv'\n",
    "results_df.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"'{output_filename}' 파일로 결과가 성공적으로 저장되었습니다.\")\n",
    "print(\"\\n=== 최종 결과 미리보기 ===\")\n",
    "print(results_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a30a76c-349d-4732-8f67-c063fa66edd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터 로드를 시작합니다.\n",
      "'/Users/younghohuh/Downloads/cifar-10/train'에서 이미지 로딩 시작...\n",
      "5000 / 50000 개 이미지 로드 완료...\n",
      "10000 / 50000 개 이미지 로드 완료...\n",
      "15000 / 50000 개 이미지 로드 완료...\n",
      "20000 / 50000 개 이미지 로드 완료...\n",
      "25000 / 50000 개 이미지 로드 완료...\n",
      "30000 / 50000 개 이미지 로드 완료...\n",
      "35000 / 50000 개 이미지 로드 완료...\n",
      "40000 / 50000 개 이미지 로드 완료...\n",
      "45000 / 50000 개 이미지 로드 완료...\n",
      "50000 / 50000 개 이미지 로드 완료...\n",
      "이미지 로딩 완료.\n",
      "'/Users/younghohuh/Downloads/cifar-10/test'에서 이미지 로딩 시작...\n",
      "5000 / 300000 개 이미지 로드 완료...\n",
      "10000 / 300000 개 이미지 로드 완료...\n",
      "15000 / 300000 개 이미지 로드 완료...\n",
      "20000 / 300000 개 이미지 로드 완료...\n",
      "25000 / 300000 개 이미지 로드 완료...\n",
      "30000 / 300000 개 이미지 로드 완료...\n",
      "35000 / 300000 개 이미지 로드 완료...\n",
      "40000 / 300000 개 이미지 로드 완료...\n",
      "45000 / 300000 개 이미지 로드 완료...\n",
      "50000 / 300000 개 이미지 로드 완료...\n",
      "55000 / 300000 개 이미지 로드 완료...\n",
      "60000 / 300000 개 이미지 로드 완료...\n",
      "65000 / 300000 개 이미지 로드 완료...\n",
      "70000 / 300000 개 이미지 로드 완료...\n",
      "75000 / 300000 개 이미지 로드 완료...\n",
      "80000 / 300000 개 이미지 로드 완료...\n",
      "85000 / 300000 개 이미지 로드 완료...\n",
      "90000 / 300000 개 이미지 로드 완료...\n",
      "95000 / 300000 개 이미지 로드 완료...\n",
      "100000 / 300000 개 이미지 로드 완료...\n",
      "105000 / 300000 개 이미지 로드 완료...\n",
      "110000 / 300000 개 이미지 로드 완료...\n",
      "115000 / 300000 개 이미지 로드 완료...\n",
      "120000 / 300000 개 이미지 로드 완료...\n",
      "125000 / 300000 개 이미지 로드 완료...\n",
      "130000 / 300000 개 이미지 로드 완료...\n",
      "135000 / 300000 개 이미지 로드 완료...\n",
      "140000 / 300000 개 이미지 로드 완료...\n",
      "145000 / 300000 개 이미지 로드 완료...\n",
      "150000 / 300000 개 이미지 로드 완료...\n",
      "155000 / 300000 개 이미지 로드 완료...\n",
      "160000 / 300000 개 이미지 로드 완료...\n",
      "165000 / 300000 개 이미지 로드 완료...\n",
      "170000 / 300000 개 이미지 로드 완료...\n",
      "175000 / 300000 개 이미지 로드 완료...\n",
      "180000 / 300000 개 이미지 로드 완료...\n",
      "185000 / 300000 개 이미지 로드 완료...\n",
      "190000 / 300000 개 이미지 로드 완료...\n",
      "195000 / 300000 개 이미지 로드 완료...\n",
      "200000 / 300000 개 이미지 로드 완료...\n",
      "205000 / 300000 개 이미지 로드 완료...\n",
      "210000 / 300000 개 이미지 로드 완료...\n",
      "215000 / 300000 개 이미지 로드 완료...\n",
      "220000 / 300000 개 이미지 로드 완료...\n",
      "225000 / 300000 개 이미지 로드 완료...\n",
      "230000 / 300000 개 이미지 로드 완료...\n",
      "235000 / 300000 개 이미지 로드 완료...\n",
      "240000 / 300000 개 이미지 로드 완료...\n",
      "245000 / 300000 개 이미지 로드 완료...\n",
      "250000 / 300000 개 이미지 로드 완료...\n",
      "255000 / 300000 개 이미지 로드 완료...\n",
      "260000 / 300000 개 이미지 로드 완료...\n",
      "265000 / 300000 개 이미지 로드 완료...\n",
      "270000 / 300000 개 이미지 로드 완료...\n",
      "275000 / 300000 개 이미지 로드 완료...\n",
      "280000 / 300000 개 이미지 로드 완료...\n",
      "285000 / 300000 개 이미지 로드 완료...\n",
      "290000 / 300000 개 이미지 로드 완료...\n",
      "295000 / 300000 개 이미지 로드 완료...\n",
      "300000 / 300000 개 이미지 로드 완료...\n",
      "이미지 로딩 완료.\n",
      "------------------------------\n",
      "데이터 정규화를 진행합니다...\n",
      "학습 데이터 형태: (50000, 3072)\n",
      "테스트 데이터 형태: (300000, 3072)\n",
      "------------------------------\n",
      "PCA 특징 추출을 시작합니다... (시간이 소요될 수 있습니다)\n",
      "PCA 변환 완료! (소요 시간: 29.77초)\n",
      "축소된 학습 데이터 형태: (50000, 150)\n",
      "축소된 테스트 데이터 형태: (300000, 150)\n",
      "------------------------------\n",
      "최적의 k를 찾기 위해 검증을 시작합니다...\n",
      "k = 1 모델 테스트 중...\n",
      "-> k = 1일 때의 검증 정확도: 37.52%\n",
      "k = 3 모델 테스트 중...\n",
      "-> k = 3일 때의 검증 정확도: 35.98%\n",
      "k = 5 모델 테스트 중...\n",
      "-> k = 5일 때의 검증 정확도: 36.56%\n",
      "k = 7 모델 테스트 중...\n",
      "-> k = 7일 때의 검증 정확도: 36.82%\n",
      "k = 9 모델 테스트 중...\n",
      "-> k = 9일 때의 검증 정확도: 35.94%\n",
      "k = 11 모델 테스트 중...\n",
      "-> k = 11일 때의 검증 정확도: 36.16%\n",
      "k = 13 모델 테스트 중...\n",
      "-> k = 13일 때의 검증 정확도: 36.52%\n",
      "------------------------------\n",
      "최적의 k를 찾았습니다: k = 1 (검증 정확도: 37.52%)\n",
      "------------------------------\n",
      "최적의 k=1를 사용하여 전체 데이터로 최종 모델을 학습 및 예측합니다.\n",
      "이 과정은 매우 오래 걸릴 수 있으니 잠시 기다려주세요...\n",
      "최종 예측 완료!\n",
      "------------------------------\n",
      "예측 결과를 CSV 파일로 저장합니다...\n",
      "'cifar10_predictions_final.csv' 파일로 결과가 성공적으로 저장되었습니다.\n",
      "\n",
      "=== 최종 결과 미리보기 ===\n",
      "   id     label\n",
      "0   1      deer\n",
      "1   2  airplane\n",
      "2   3     horse\n",
      "3   4      ship\n",
      "4   5      ship\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# --- 1. 경로 설정 ---\n",
    "# 사용자께서 제공해주신 경로를 설정합니다.\n",
    "TRAIN_DATA_PATH = '/Users/younghohuh/Downloads/cifar-10/train'\n",
    "TEST_DATA_PATH = '/Users/younghohuh/Downloads/cifar-10/test'\n",
    "LABEL_CSV_PATH = '/Users/younghohuh/Downloads/cifar-10/trainLabels.csv'  # Notebook과 같은 폴더에 있다고 가정\n",
    "OUTPUT_CSV_PATH = 'cifar10_predictions_final.csv'\n",
    "\n",
    "# 클래스 이름 정의 (CIFAR-10 표준 순서)\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "label_to_id = {label: i for i, label in enumerate(class_names)}\n",
    "id_to_label = {i: label for i, label in enumerate(class_names)}\n",
    "\n",
    "# --- 2. 이미지 데이터 로드 함수 ---\n",
    "def load_images_from_folder(folder_path, file_list):\n",
    "    \"\"\"지정된 폴더에서 이미지 파일들을 순서대로 로드하여 NumPy 배열로 반환\"\"\"\n",
    "    images = []\n",
    "    print(f\"'{folder_path}'에서 이미지 로딩 시작...\")\n",
    "    \n",
    "    for i, filename in enumerate(file_list):\n",
    "        if (i + 1) % 5000 == 0:\n",
    "            print(f\"{i + 1} / {len(file_list)} 개 이미지 로드 완료...\")\n",
    "            \n",
    "        img_path = os.path.join(folder_path, filename)\n",
    "        with Image.open(img_path) as img:\n",
    "            # 이미지를 NumPy 배열로 변환하고 1차원으로 펼침 (32x32x3 -> 3072)\n",
    "            images.append(np.array(img).flatten())\n",
    "            \n",
    "    print(\"이미지 로딩 완료.\")\n",
    "    return np.array(images)\n",
    "\n",
    "# --- 3. 데이터 로드 및 전처리 ---\n",
    "print(\"학습 데이터 로드를 시작합니다.\")\n",
    "# 레이블 파일 로드\n",
    "try:\n",
    "    labels_df = pd.read_csv(LABEL_CSV_PATH)\n",
    "except FileNotFoundError:\n",
    "    print(f\"'{LABEL_CSV_PATH}' 파일을 찾을 수 없습니다. 경로를 확인해주세요.\")\n",
    "    # 이 경우 실행을 중단합니다.\n",
    "    exit()\n",
    "\n",
    "# 파일 이름 순서가 id 순서와 일치하도록 정렬\n",
    "train_filenames = sorted(os.listdir(TRAIN_DATA_PATH), key=lambda x: int(x.split('.')[0]))\n",
    "test_filenames = sorted(os.listdir(TEST_DATA_PATH), key=lambda x: int(x.split('.')[0]))\n",
    "\n",
    "# 학습 이미지와 레이블 로드\n",
    "X_train_raw = load_images_from_folder(TRAIN_DATA_PATH, train_filenames)\n",
    "# CSV 파일의 레이블을 숫자 ID로 변환\n",
    "y_train_labels = labels_df['label'].map(label_to_id).values\n",
    "\n",
    "# 테스트 이미지 로드\n",
    "X_test_raw = load_images_from_folder(TEST_DATA_PATH, test_filenames)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(\"데이터 정규화를 진행합니다...\")\n",
    "# 픽셀 값을 0과 1 사이로 정규화\n",
    "X_train_normalized = X_train_raw / 255.0\n",
    "X_test_normalized = X_test_raw / 255.0\n",
    "print(f\"학습 데이터 형태: {X_train_normalized.shape}\")\n",
    "print(f\"테스트 데이터 형태: {X_test_normalized.shape}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- 4. PCA를 이용한 특징 추출 ---\n",
    "print(\"PCA 특징 추출을 시작합니다... (시간이 소요될 수 있습니다)\")\n",
    "pca = PCA(n_components=150)\n",
    "\n",
    "start_time = time.time()\n",
    "# 중요: PCA는 반드시 학습 데이터에만 fit 해야 합니다.\n",
    "pca.fit(X_train_normalized)\n",
    "X_train_pca = pca.transform(X_train_normalized)\n",
    "X_test_pca = pca.transform(X_test_normalized)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"PCA 변환 완료! (소요 시간: {end_time - start_time:.2f}초)\")\n",
    "print(f\"축소된 학습 데이터 형태: {X_train_pca.shape}\")\n",
    "print(f\"축소된 테스트 데이터 형태: {X_test_pca.shape}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- 5. 최적의 k 찾기 (홀수 k만 사용) ---\n",
    "print(\"최적의 k를 찾기 위해 검증을 시작합니다...\")\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    X_train_pca, y_train_labels, test_size=0.1, random_state=42, stratify=y_train_labels\n",
    ")\n",
    "\n",
    "best_k = 0\n",
    "best_accuracy = 0\n",
    "k_values = range(1, 14, 2)  # 1, 3, 5, ..., 13\n",
    "\n",
    "for k in k_values:\n",
    "    print(f\"k = {k} 모델 테스트 중...\")\n",
    "    knn_temp = KNeighborsClassifier(n_neighbors=k, n_jobs=-1)\n",
    "    \n",
    "    knn_temp.fit(X_train_split, y_train_split)\n",
    "    y_val_pred = knn_temp.predict(X_val_split)\n",
    "    accuracy = accuracy_score(y_val_split, y_val_pred)\n",
    "    \n",
    "    print(f\"-> k = {k}일 때의 검증 정확도: {accuracy * 100:.2f}%\")\n",
    "    \n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_k = k\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"최적의 k를 찾았습니다: k = {best_k} (검증 정확도: {best_accuracy * 100:.2f}%)\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- 6. 전체 데이터로 최종 모델 학습 및 예측 ---\n",
    "print(f\"최적의 k={best_k}를 사용하여 전체 데이터로 최종 모델을 학습 및 예측합니다.\")\n",
    "print(\"이 과정은 매우 오래 걸릴 수 있으니 잠시 기다려주세요...\")\n",
    "\n",
    "final_knn_classifier = KNeighborsClassifier(n_neighbors=best_k, n_jobs=-1)\n",
    "\n",
    "# 전체 학습 데이터로 다시 학습\n",
    "final_knn_classifier.fit(X_train_pca, y_train_labels)\n",
    "\n",
    "# 테스트 데이터로 예측\n",
    "final_predictions_numeric = final_knn_classifier.predict(X_test_pca)\n",
    "\n",
    "print(\"최종 예측 완료!\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- 7. 결과 CSV 파일로 저장 ---\n",
    "print(\"예측 결과를 CSV 파일로 저장합니다...\")\n",
    "# 테스트 파일의 id(이름에서 확장자 제외)를 추출\n",
    "test_ids = [int(fn.split('.')[0]) for fn in test_filenames]\n",
    "# 숫자 레이블을 다시 클래스 이름으로 변환\n",
    "predicted_class_names = [id_to_label[i] for i in final_predictions_numeric]\n",
    "\n",
    "# 최종 결과 DataFrame 생성\n",
    "final_df = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'label': predicted_class_names\n",
    "})\n",
    "\n",
    "# id 순서로 정렬 후 저장\n",
    "final_df.sort_values(by='id', inplace=True)\n",
    "final_df.to_csv(OUTPUT_CSV_PATH, index=False)\n",
    "\n",
    "print(f\"'{OUTPUT_CSV_PATH}' 파일로 결과가 성공적으로 저장되었습니다.\")\n",
    "print(\"\\n=== 최종 결과 미리보기 ===\")\n",
    "print(final_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4032cd26-3a5e-43ba-93f0-d1a99b051b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 1. 데이터 로드 및 분할 ###\n",
      "'train' 폴더에서 총 50000개의 이미지 로드를 시작합니다...\n",
      "... 1000 / 50000 개 로드 완료\n",
      "... 2000 / 50000 개 로드 완료\n",
      "... 3000 / 50000 개 로드 완료\n",
      "... 4000 / 50000 개 로드 완료\n",
      "... 5000 / 50000 개 로드 완료\n",
      "... 6000 / 50000 개 로드 완료\n",
      "... 7000 / 50000 개 로드 완료\n",
      "... 8000 / 50000 개 로드 완료\n",
      "... 9000 / 50000 개 로드 완료\n",
      "... 10000 / 50000 개 로드 완료\n",
      "... 11000 / 50000 개 로드 완료\n",
      "... 12000 / 50000 개 로드 완료\n",
      "... 13000 / 50000 개 로드 완료\n",
      "... 14000 / 50000 개 로드 완료\n",
      "... 15000 / 50000 개 로드 완료\n",
      "... 16000 / 50000 개 로드 완료\n",
      "... 17000 / 50000 개 로드 완료\n",
      "... 18000 / 50000 개 로드 완료\n",
      "... 19000 / 50000 개 로드 완료\n",
      "... 20000 / 50000 개 로드 완료\n",
      "... 21000 / 50000 개 로드 완료\n",
      "... 22000 / 50000 개 로드 완료\n",
      "... 23000 / 50000 개 로드 완료\n",
      "... 24000 / 50000 개 로드 완료\n",
      "... 25000 / 50000 개 로드 완료\n",
      "... 26000 / 50000 개 로드 완료\n",
      "... 27000 / 50000 개 로드 완료\n",
      "... 28000 / 50000 개 로드 완료\n",
      "... 29000 / 50000 개 로드 완료\n",
      "... 30000 / 50000 개 로드 완료\n",
      "... 31000 / 50000 개 로드 완료\n",
      "... 32000 / 50000 개 로드 완료\n",
      "... 33000 / 50000 개 로드 완료\n",
      "... 34000 / 50000 개 로드 완료\n",
      "... 35000 / 50000 개 로드 완료\n",
      "... 36000 / 50000 개 로드 완료\n",
      "... 37000 / 50000 개 로드 완료\n",
      "... 38000 / 50000 개 로드 완료\n",
      "... 39000 / 50000 개 로드 완료\n",
      "... 40000 / 50000 개 로드 완료\n",
      "... 41000 / 50000 개 로드 완료\n",
      "... 42000 / 50000 개 로드 완료\n",
      "... 43000 / 50000 개 로드 완료\n",
      "... 44000 / 50000 개 로드 완료\n",
      "... 45000 / 50000 개 로드 완료\n",
      "... 46000 / 50000 개 로드 완료\n",
      "... 47000 / 50000 개 로드 완료\n",
      "... 48000 / 50000 개 로드 완료\n",
      "... 49000 / 50000 개 로드 완료\n",
      "... 50000 / 50000 개 로드 완료\n",
      "'train' 폴더 이미지 로딩 완료.\n",
      "'test' 폴더에서 총 1000개의 이미지 로드를 시작합니다...\n",
      "... 1000 / 1000 개 로드 완료\n",
      "'test' 폴더 이미지 로딩 완료.\n",
      "\n",
      "데이터를 Train, Validation 세트로 분할합니다...\n",
      "\n",
      "데이터 정규화를 진행합니다...\n",
      "  - 학습(Train) 데이터: (45000, 3072)\n",
      "  - 검증(Validation) 데이터: (5000, 3072)\n",
      "  - 테스트(Test) 데이터: (1000, 3072)\n",
      "------------------------------\n",
      "\n",
      "### 2. 최적의 k 찾기 (1, 3, 5...13) ###\n",
      "\n",
      "_k = 1_ 모델 테스트 중...\n",
      "-> k = 1 검증 정확도: 34.56% (소요 시간: 7.61초)\n",
      "\n",
      "_k = 3_ 모델 테스트 중...\n",
      "-> k = 3 검증 정확도: 33.30% (소요 시간: 7.87초)\n",
      "\n",
      "_k = 5_ 모델 테스트 중...\n",
      "-> k = 5 검증 정확도: 34.40% (소요 시간: 8.15초)\n",
      "\n",
      "_k = 7_ 모델 테스트 중...\n",
      "-> k = 7 검증 정확도: 34.34% (소요 시간: 8.25초)\n",
      "\n",
      "_k = 9_ 모델 테스트 중...\n",
      "-> k = 9 검증 정확도: 34.04% (소요 시간: 8.46초)\n",
      "\n",
      "_k = 11_ 모델 테스트 중...\n",
      "-> k = 11 검증 정확도: 33.58% (소요 시간: 8.38초)\n",
      "\n",
      "_k = 13_ 모델 테스트 중...\n",
      "-> k = 13 검증 정확도: 33.34% (소요 시간: 8.70초)\n",
      "------------------------------\n",
      "최적의 k를 찾았습니다: k = 1 (최고 검증 정확도: 34.56%)\n",
      "------------------------------\n",
      "\n",
      "### 3. 최종 예측 (k=1) ###\n",
      "1000개의 테스트 데이터에 대한 예측을 시작합니다...\n",
      "최종 예측 완료! (소요 시간: 1.71초)\n",
      "------------------------------\n",
      "\n",
      "### 4. 결과 저장 ###\n",
      "'cifar10_predictions_1000_test.csv' 파일로 결과가 성공적으로 저장되었습니다.\n",
      "\n",
      "=== 최종 결과 미리보기 ===\n",
      "   id     label\n",
      "0   1       dog\n",
      "1   2  airplane\n",
      "2   3      bird\n",
      "3   4      ship\n",
      "4   5      ship\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# --- 1. 경로 및 설정 ---\n",
    "TRAIN_DATA_PATH = '/Users/younghohuh/Downloads/cifar-10/train'\n",
    "TEST_DATA_PATH = '/Users/younghohuh/Downloads/cifar-10/test'\n",
    "LABEL_CSV_PATH = '/Users/younghohuh/Downloads/cifar-10/trainLabels.csv'\n",
    "OUTPUT_CSV_PATH = 'cifar10_predictions_1000_test.csv'\n",
    "\n",
    "NUM_TEST_SAMPLES = 1000  # 테스트할 이미지 개수\n",
    "\n",
    "# 클래스 이름 및 레이블 매핑 정의\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "label_to_id = {label: i for i, label in enumerate(class_names)}\n",
    "id_to_label = {i: label for i, label in enumerate(class_names)}\n",
    "\n",
    "# --- 2. 이미지 데이터 로드 함수 ---\n",
    "def load_images_from_folder(folder_path, file_list):\n",
    "    \"\"\"지정된 폴더에서 이미지 파일들을 순서대로 로드하여 NumPy 배열로 반환\"\"\"\n",
    "    images = []\n",
    "    total_files = len(file_list)\n",
    "    folder_name = os.path.basename(folder_path)\n",
    "    print(f\"'{folder_name}' 폴더에서 총 {total_files}개의 이미지 로드를 시작합니다...\")\n",
    "    \n",
    "    for i, filename in enumerate(file_list):\n",
    "        if (i + 1) % 1000 == 0:\n",
    "            print(f\"... {i + 1} / {total_files} 개 로드 완료\")\n",
    "            \n",
    "        img_path = os.path.join(folder_path, filename)\n",
    "        with Image.open(img_path) as img:\n",
    "            images.append(np.array(img).flatten())\n",
    "            \n",
    "    print(f\"'{folder_name}' 폴더 이미지 로딩 완료.\")\n",
    "    return np.array(images)\n",
    "\n",
    "# --- 3. 데이터 로드 및 분할 ---\n",
    "print(\"### 1. 데이터 로드 및 분할 ###\")\n",
    "try:\n",
    "    labels_df = pd.read_csv(LABEL_CSV_PATH)\n",
    "except FileNotFoundError:\n",
    "    print(f\"오류: '{LABEL_CSV_PATH}' 파일을 찾을 수 없습니다.\")\n",
    "    print(\"Jupyter Notebook과 같은 폴더에 해당 파일이 있는지 확인해주세요.\")\n",
    "    exit()\n",
    "\n",
    "# 전체 학습 파일 목록 (50,000개)\n",
    "train_filenames = sorted(os.listdir(TRAIN_DATA_PATH), key=lambda x: int(x.split('.')[0]))\n",
    "X_train_full_raw = load_images_from_folder(TRAIN_DATA_PATH, train_filenames)\n",
    "y_train_full_labels = labels_df.sort_values('id')['label'].map(label_to_id).values\n",
    "\n",
    "# 전체 테스트 파일 목록에서 1000개만 선택\n",
    "test_filenames_all = sorted(os.listdir(TEST_DATA_PATH), key=lambda x: int(x.split('.')[0]))\n",
    "test_filenames_subset = test_filenames_all[:NUM_TEST_SAMPLES]\n",
    "X_test_raw = load_images_from_folder(TEST_DATA_PATH, test_filenames_subset)\n",
    "\n",
    "print(\"\\n데이터를 Train, Validation 세트로 분할합니다...\")\n",
    "# 학습 데이터를 학습용(45,000)과 검증용(5,000)으로 분할\n",
    "# stratify: 분할 후에도 각 클래스의 비율을 원래대로 유지해주는 중요한 옵션\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_full_raw, y_train_full_labels, \n",
    "    test_size=5000, \n",
    "    random_state=42, \n",
    "    stratify=y_train_full_labels\n",
    ")\n",
    "\n",
    "print(\"\\n데이터 정규화를 진행합니다...\")\n",
    "# 픽셀 값을 0과 1 사이로 정규화\n",
    "X_train = X_train / 255.0\n",
    "X_val = X_val / 255.0\n",
    "X_test = X_test_raw / 255.0\n",
    "\n",
    "print(f\"  - 학습(Train) 데이터: {X_train.shape}\")\n",
    "print(f\"  - 검증(Validation) 데이터: {X_val.shape}\")\n",
    "print(f\"  - 테스트(Test) 데이터: {X_test.shape}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# --- 4. 최적의 k 찾기 (검증 데이터 사용) ---\n",
    "print(\"\\n### 2. 최적의 k 찾기 (1, 3, 5...13) ###\")\n",
    "best_k = 0\n",
    "best_accuracy = 0\n",
    "k_values = range(1, 14, 2)\n",
    "\n",
    "for k in k_values:\n",
    "    print(f\"\\n_k = {k}_ 모델 테스트 중...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # n_jobs=-1 : CPU 모든 코어를 사용하여 계산 속도 향상\n",
    "    knn_temp = KNeighborsClassifier(n_neighbors=k, n_jobs=-1)\n",
    "    \n",
    "    # 학습 데이터로 모델 학습\n",
    "    knn_temp.fit(X_train, y_train)\n",
    "    \n",
    "    # 검증 데이터로 성능 측정\n",
    "    y_val_pred = knn_temp.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"-> k = {k} 검증 정확도: {accuracy * 100:.2f}% (소요 시간: {end_time - start_time:.2f}초)\")\n",
    "    \n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_k = k\n",
    "        # 최적 모델을 저장해둠\n",
    "        final_knn_classifier = knn_temp\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"최적의 k를 찾았습니다: k = {best_k} (최고 검증 정확도: {best_accuracy * 100:.2f}%)\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# --- 5. 최종 예측 (테스트 데이터 사용) ---\n",
    "print(f\"\\n### 3. 최종 예측 (k={best_k}) ###\")\n",
    "print(f\"{NUM_TEST_SAMPLES}개의 테스트 데이터에 대한 예측을 시작합니다...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# 위에서 찾은 최적의 모델(final_knn_classifier)로 바로 예측 수행\n",
    "final_predictions_numeric = final_knn_classifier.predict(X_test)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"최종 예측 완료! (소요 시간: {end_time - start_time:.2f}초)\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# --- 6. 결과 CSV 파일로 저장 ---\n",
    "print(\"\\n### 4. 결과 저장 ###\")\n",
    "test_ids = [int(fn.split('.')[0]) for fn in test_filenames_subset]\n",
    "predicted_class_names = [id_to_label[i] for i in final_predictions_numeric]\n",
    "\n",
    "final_df = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'label': predicted_class_names\n",
    "})\n",
    "\n",
    "final_df.sort_values(by='id', inplace=True)\n",
    "final_df.to_csv(OUTPUT_CSV_PATH, index=False)\n",
    "\n",
    "print(f\"'{OUTPUT_CSV_PATH}' 파일로 결과가 성공적으로 저장되었습니다.\")\n",
    "print(\"\\n=== 최종 결과 미리보기 ===\")\n",
    "print(final_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38d055fa-2277-4468-8dc9-80f0a4ad7e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### 1. 데이터 로딩 및 전처리 ###\n",
      "'train' 폴더에서 총 50000개의 이미지 로드를 시작합니다...\n",
      "... 5000 / 50000 개 로드 완료\n",
      "... 10000 / 50000 개 로드 완료\n",
      "... 15000 / 50000 개 로드 완료\n",
      "... 20000 / 50000 개 로드 완료\n",
      "... 25000 / 50000 개 로드 완료\n",
      "... 30000 / 50000 개 로드 완료\n",
      "... 35000 / 50000 개 로드 완료\n",
      "... 40000 / 50000 개 로드 완료\n",
      "... 45000 / 50000 개 로드 완료\n",
      "... 50000 / 50000 개 로드 완료\n",
      "'train' 폴더 이미지 로딩 완료.\n",
      "'test' 폴더에서 총 1000개의 이미지 로드를 시작합니다...\n",
      "'test' 폴더 이미지 로딩 완료.\n",
      "\n",
      "데이터 정규화를 진행합니다...\n",
      "  - 전체 학습 데이터: (50000, 3072)\n",
      "  - 테스트 데이터: (1000, 3072)\n",
      "------------------------------\n",
      "\n",
      "### 2. 최적의 k 찾기 (5-Fold Cross-Validation, 10000개 샘플 사용) ###\n",
      "경고: 이 과정은 이전보다 시간이 더 소요됩니다.\n",
      "\n",
      "_k = 1_ 교차 검증 중...\n",
      "-> k = 1의 평균 검증 정확도: 28.23% (소요 시간: 4.44초)\n",
      "\n",
      "_k = 3_ 교차 검증 중...\n",
      "-> k = 3의 평균 검증 정확도: 27.74% (소요 시간: 4.18초)\n",
      "\n",
      "_k = 5_ 교차 검증 중...\n",
      "-> k = 5의 평균 검증 정확도: 28.88% (소요 시간: 3.64초)\n",
      "\n",
      "_k = 7_ 교차 검증 중...\n",
      "-> k = 7의 평균 검증 정확도: 28.75% (소요 시간: 3.87초)\n",
      "\n",
      "_k = 9_ 교차 검증 중...\n",
      "-> k = 9의 평균 검증 정확도: 28.87% (소요 시간: 3.72초)\n",
      "\n",
      "_k = 11_ 교차 검증 중...\n",
      "-> k = 11의 평균 검증 정확도: 28.68% (소요 시간: 3.86초)\n",
      "\n",
      "_k = 13_ 교차 검증 중...\n",
      "-> k = 13의 평균 검증 정확도: 28.79% (소요 시간: 3.94초)\n",
      "------------------------------\n",
      "최적의 k를 찾았습니다: k = 5 (최고 평균 검증 정확도: 28.88%)\n",
      "------------------------------\n",
      "\n",
      "### 3. 최종 모델 학습 및 예측 (k=5) ###\n",
      "전체 50,000개 데이터로 최종 모델을 학습합니다... (시간 소요)\n",
      "1,000개의 테스트 데이터에 대한 예측을 시작합니다...\n",
      "최종 예측 완료! (소요 시간: 1.94초)\n",
      "------------------------------\n",
      "\n",
      "### 4. 결과 저장 ###\n",
      "'cifar10_predictions_kfold.csv' 파일로 결과가 성공적으로 저장되었습니다.\n",
      "\n",
      "=== 최종 결과 미리보기 ===\n",
      "   id     label\n",
      "0   1      frog\n",
      "1   2  airplane\n",
      "2   3     horse\n",
      "3   4      ship\n",
      "4   5      bird\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# --- 1. 경로 및 설정 ---\n",
    "TRAIN_DATA_PATH = '/Users/younghohuh/Downloads/cifar-10/train'\n",
    "TEST_DATA_PATH = '/Users/younghohuh/Downloads/cifar-10/test'\n",
    "LABEL_CSV_PATH = '/Users/younghohuh/Downloads/cifar-10/trainLabels.csv'\n",
    "OUTPUT_CSV_PATH = 'cifar10_predictions_kfold.csv'\n",
    "\n",
    "NUM_SAMPLES_FOR_CV = 10000 # 교차 검증에 사용할 샘플 데이터 개수 (시간 단축용)\n",
    "NUM_TEST_SAMPLES = 1000      # 테스트할 이미지 개수\n",
    "\n",
    "# 클래스 이름 및 레이블 매핑 정의\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "label_to_id = {label: i for i, label in enumerate(class_names)}\n",
    "id_to_label = {i: label for i, label in enumerate(class_names)}\n",
    "\n",
    "# --- 2. 이미지 데이터 로드 함수 ---\n",
    "def load_images_from_folder(folder_path, file_list):\n",
    "    \"\"\"지정된 폴더에서 이미지 파일들을 순서대로 로드하여 NumPy 배열로 반환\"\"\"\n",
    "    images = []\n",
    "    total_files = len(file_list)\n",
    "    folder_name = os.path.basename(folder_path)\n",
    "    print(f\"'{folder_name}' 폴더에서 총 {total_files}개의 이미지 로드를 시작합니다...\")\n",
    "    \n",
    "    for i, filename in enumerate(file_list):\n",
    "        if (i + 1) % 5000 == 0:\n",
    "            print(f\"... {i + 1} / {total_files} 개 로드 완료\")\n",
    "            \n",
    "        img_path = os.path.join(folder_path, filename)\n",
    "        with Image.open(img_path) as img:\n",
    "            images.append(np.array(img).flatten())\n",
    "            \n",
    "    print(f\"'{folder_name}' 폴더 이미지 로딩 완료.\")\n",
    "    return np.array(images)\n",
    "\n",
    "# --- 3. 데이터 로드 및 전처리 ---\n",
    "print(\"### 1. 데이터 로딩 및 전처리 ###\")\n",
    "try:\n",
    "    labels_df = pd.read_csv(LABEL_CSV_PATH)\n",
    "except FileNotFoundError:\n",
    "    print(f\"오류: '{LABEL_CSV_PATH}' 파일을 찾을 수 없습니다. 경로를 확인해주세요.\")\n",
    "    exit()\n",
    "\n",
    "# 전체 학습 데이터 로드\n",
    "train_filenames = sorted(os.listdir(TRAIN_DATA_PATH), key=lambda x: int(x.split('.')[0]))\n",
    "X_train_full_raw = load_images_from_folder(TRAIN_DATA_PATH, train_filenames)\n",
    "y_train_full_labels = labels_df.sort_values('id')['label'].map(label_to_id).values\n",
    "\n",
    "# 테스트 데이터 1,000개만 로드\n",
    "test_filenames_all = sorted(os.listdir(TEST_DATA_PATH), key=lambda x: int(x.split('.')[0]))\n",
    "test_filenames_subset = test_filenames_all[:NUM_TEST_SAMPLES]\n",
    "X_test_raw = load_images_from_folder(TEST_DATA_PATH, test_filenames_subset)\n",
    "\n",
    "print(\"\\n데이터 정규화를 진행합니다...\")\n",
    "X_train_full_normalized = X_train_full_raw / 255.0\n",
    "X_test_normalized = X_test_raw / 255.0\n",
    "\n",
    "print(f\"  - 전체 학습 데이터: {X_train_full_normalized.shape}\")\n",
    "print(f\"  - 테스트 데이터: {X_test_normalized.shape}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- 4. 최적의 k 찾기 (5-Fold Cross-Validation) ---\n",
    "print(f\"\\n### 2. 최적의 k 찾기 (5-Fold Cross-Validation, {NUM_SAMPLES_FOR_CV}개 샘플 사용) ###\")\n",
    "print(\"경고: 이 과정은 이전보다 시간이 더 소요됩니다.\")\n",
    "\n",
    "# 교차 검증을 위한 데이터 샘플링 (시간 단축)\n",
    "np.random.seed(42) # 항상 같은 샘플을 뽑기 위해 시드 고정\n",
    "cv_indices = np.random.choice(X_train_full_normalized.shape[0], NUM_SAMPLES_FOR_CV, replace=False)\n",
    "X_cv_subset = X_train_full_normalized[cv_indices]\n",
    "y_cv_subset = y_train_full_labels[cv_indices]\n",
    "\n",
    "best_k = 0\n",
    "best_accuracy = 0\n",
    "k_values = range(1, 14, 2)  # 1, 3, 5, ..., 13\n",
    "\n",
    "for k in k_values:\n",
    "    print(f\"\\n_k = {k}_ 교차 검증 중...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    knn_temp = KNeighborsClassifier(n_neighbors=k, n_jobs=-1)\n",
    "    \n",
    "    # cv=5 : 5-Fold Cross-Validation을 의미\n",
    "    # cross_val_score가 알아서 데이터를 5개로 나누고 학습-평가를 5번 반복합니다.\n",
    "    scores = cross_val_score(knn_temp, X_cv_subset, y_cv_subset, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    \n",
    "    average_accuracy = np.mean(scores)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"-> k = {k}의 평균 검증 정확도: {average_accuracy * 100:.2f}% (소요 시간: {end_time - start_time:.2f}초)\")\n",
    "    \n",
    "    if average_accuracy > best_accuracy:\n",
    "        best_accuracy = average_accuracy\n",
    "        best_k = k\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"최적의 k를 찾았습니다: k = {best_k} (최고 평균 검증 정확도: {best_accuracy * 100:.2f}%)\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- 5. 전체 학습 데이터로 최종 모델 학습 및 예측 ---\n",
    "print(f\"\\n### 3. 최종 모델 학습 및 예측 (k={best_k}) ###\")\n",
    "print(\"전체 50,000개 데이터로 최종 모델을 학습합니다... (시간 소요)\")\n",
    "final_knn_classifier = KNeighborsClassifier(n_neighbors=best_k, n_jobs=-1)\n",
    "\n",
    "# ★ 전체 학습 데이터로 다시 학습\n",
    "final_knn_classifier.fit(X_train_full_normalized, y_train_full_labels)\n",
    "\n",
    "print(\"1,000개의 테스트 데이터에 대한 예측을 시작합니다...\")\n",
    "start_time = time.time()\n",
    "final_predictions_numeric = final_knn_classifier.predict(X_test_normalized)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"최종 예측 완료! (소요 시간: {end_time - start_time:.2f}초)\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- 6. 결과 CSV 파일로 저장 ---\n",
    "print(\"\\n### 4. 결과 저장 ###\")\n",
    "test_ids = [int(fn.split('.')[0]) for fn in test_filenames_subset]\n",
    "predicted_class_names = [id_to_label[i] for i in final_predictions_numeric]\n",
    "\n",
    "final_df = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'label': predicted_class_names\n",
    "})\n",
    "\n",
    "final_df.sort_values(by='id', inplace=True)\n",
    "final_df.to_csv(OUTPUT_CSV_PATH, index=False)\n",
    "\n",
    "print(f\"'{OUTPUT_CSV_PATH}' 파일로 결과가 성공적으로 저장되었습니다.\")\n",
    "print(\"\\n=== 최종 결과 미리보기 ===\")\n",
    "print(final_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b025923-909c-4776-891d-9cffd77fa9e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9221fbff-d254-4b0c-bc86-a5b6c867d134",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
